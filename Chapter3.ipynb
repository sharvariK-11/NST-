{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sharvariK-11/NST-/blob/main/Chapter3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnzbA9c9K8Pq"
      },
      "source": [
        "### Chapter 3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbVQiEElLCLF"
      },
      "source": [
        "downloading the datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Un1sudR5MYA1"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "import transformers\n",
        "!pip install transformers[sentencepiece]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFutRr2NN6Jt"
      },
      "outputs": [],
      "source": [
        "!pip3 install datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2NgKiccLdHE"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsTHvSwTVHye"
      },
      "outputs": [],
      "source": [
        "raw_train_dataset=raw_datasets[\"train\"]\n",
        "raw_train_dataset[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z10iv9srb4JW"
      },
      "outputs": [],
      "source": [
        "raw_datasets[\"train\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzNHrKUSQAAx"
      },
      "outputs": [],
      "source": [
        "raw_datasets[\"train\"][6]\n",
        "raw_datasets[\"train\"][:5]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxWpWBfpSPpq"
      },
      "outputs": [],
      "source": [
        "raw_train_dataset.features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehHFDZ8BcCb2"
      },
      "outputs": [],
      "source": [
        "raw_datasets[\"train\"].features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miyhy_M3W5JB"
      },
      "source": [
        "tokenise and apply functiomn over all split of dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2K4RrFFVrSX"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint=\"bert-base-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "def tokenize_function(example):\n",
        "  return tokenizer(\n",
        "      example[\"sentence1\"], example[\"sentence2\"], padding=\"max_length\", truncation=True, max_length=128\n",
        "  )\n",
        " \n",
        "tokenized_datasets=raw_datasets.map(tokenize_function, batched=True)\n",
        "tokenized_datasets=tokenized_datasets.remove_columns([\"idx\", \"sentence1\", \"sentence2\"])\n",
        "tokenized_datasets=tokenized_datasets.rename_column(\"label\", \"labels\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_HrAP14OMgf"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets=tokenized_datasets.with_format(\"torch\")\n",
        "tokenized_datasets[\"train\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipJVlGy9aJp-"
      },
      "source": [
        "generation of short sample of dataset using select method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgxELhaiaQBa"
      },
      "outputs": [],
      "source": [
        "small_train_dataset=tokenized_datasets[\"train\"].select(range(100))\n",
        "print(small_train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2QDTYbMfXfB"
      },
      "source": [
        "Pre processing the Data -o preprocess the dataset, we need to convert the text to numbers the model can make sense of. This is done with TOKENIZER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfdlvTRrgZcT"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "checkpoint=\"bert-base-uncased\"\n",
        "\n",
        "tokenizer=AutoTokenizer.from_pretrained(checkpoint)\n",
        "batch= tokenizer(\n",
        "    [\"My name is Sharvari\", \"I study at IIT Bombay\"],\n",
        "    [\"I study at IIT Bombay\", \"This coffee is hot\"], \n",
        "    padding=True, \n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "model= AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "outputs=model(**batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-hBDqq-mGhV"
      },
      "source": [
        "###Dynamic Padding\n",
        "\n",
        "\n",
        "Dynamic padding means the samples in this batch should all be padded to the maximum length inside the batch. Without dynamic padding, all of the samples would have to be padded to the maximum length in the whole dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAuMtnMOi4Zo"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "raw_datasets=load_dataset(\"glue\", \"mrpc\")\n",
        "checkpoint= \"bert-base-cased\"\n",
        "tokenizer=AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "  return tokenizer(\n",
        "      examples[\"sentence1\"], examples[\"sentence2\"], padding=\"max_length\", truncatiom=True, maximum_length=128\n",
        "  )      \n",
        "\n",
        "  tokenized_datasets=raw_datasets.map(tokenize_function, batched=True)  \n",
        "  tokenied_datasets=tokenized_datasets.remove_columns[(\"idx\", \"sentence\", \"sentence2\")]  \n",
        "  tokenized_datasets=tokenized_datasets.rename(\"label\", \"labels\")  \n",
        "  tokenized_datasets=tokenized_datasets.with_format(\"torch\")\n",
        "                    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqA0orBKqWCH"
      },
      "source": [
        "using dataset in std PyTorch DataLoader -----> Batches of fixed shapes but unnecessary paddding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxVrQXX6pL0H"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader= DataLoader(tokenized_datasets[\"train\"], batch_size=16, shuffle=True)\n",
        "\n",
        "for step, batch in enumerate(train_dataloader):\n",
        "  print(batch[\"input_ids\"].shape)\n",
        "  if step>5:\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nt22XLZjqsWo"
      },
      "source": [
        "For Dynamic Padding, postpone padding in preprocessing function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4h500npKqfWT"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "raw_datasets=load_dataset(\"glue\", \"mrpc\")\n",
        "checkpoint= \"bert-base-cased\"\n",
        "tokenizer=AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "  return tokenizer(\n",
        "      examples[\"sentence1\"], examples[\"sentence2\"], truncatiom=True\n",
        "  )      \n",
        "\n",
        "  tokenized_datasets=raw_datasets.map(tokenize_function, batched=True)  \n",
        "  tokenized_datasets=tokenized_datasets.remove_columns[(\"idx\", \"sentence\", \"sentence2\")]  \n",
        "  tokenized_datasets=tokenized_datasets.rename(\"label\", \"labels\")  \n",
        "  tokenized_datasets=tokenized_datasets.with_format(\"torch\")     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkTCDCjpsf5r"
      },
      "source": [
        "collate function- puts together samples inside a batch "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMu449FHrNYE"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator= DataCollatorWithPadding(tokenizer)\n",
        "train_dataloader =DataLoader(\n",
        "    tokenized_datasets[\"train\"], batch_size=16, shuffle=True, collate_fn=data_collator\n",
        ")\n",
        "\n",
        "for step, batch in enumerate(train_dataloader):\n",
        "  print(batch[\"input_ids\"].shape)\n",
        "  if step>5:\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jny0A9NJsNlj"
      },
      "source": [
        "Thus each batch has different but no unnecessary padding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sinHT-hq9Ed"
      },
      "source": [
        "###Fine-tuning a model with the Trainer API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFU3e6ADItAo"
      },
      "source": [
        "1. starting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5yXUXBzJq9X"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "\n",
        "raw_dataset=load_dataset(\"glue\", \"mrpc\")\n",
        "checkpoint=\"bert-base-uncased\"\n",
        "\n",
        "tokenizer=AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "def tokenize_function(example):\n",
        "  return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
        "\n",
        "tokenized_dataset=raw_dataset.map(tokenize_function, batched=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MM7rD9x3KiNT"
      },
      "source": [
        "training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvoG0H34KvSt"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args=TrainingArguments(\"test-trainer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOp-KiULLBGa"
      },
      "source": [
        "Defining model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAeDoiZFLC7j"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model=AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fz0mgS3HROt-"
      },
      "source": [
        "pass all objects into trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gm4FykvOREQ3"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mNU1z8TVZJu"
      },
      "source": [
        "fine tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "noEhd6n9VTtj",
        "outputId": "250a7169-a1ae-474a-8284-075af0bb6fc6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 3668\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1377\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='374' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 374/1377 59:43 < 2:41:01, 0.10 it/s, Epoch 0.81/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ymzl1zqVi7f"
      },
      "source": [
        "evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSyXmWAuVe4-"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_preds):\n",
        "    metric = load_metric(\"glue\", \"mrpc\")\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldp3SssCVr7A"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IH1sW52eV3P7"
      },
      "outputs": [],
      "source": [
        "tainer.train()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Chapter3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMK93NtDrbKEQRbjzP2BFg3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}